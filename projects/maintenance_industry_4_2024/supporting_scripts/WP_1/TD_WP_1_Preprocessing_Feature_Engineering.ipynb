{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exective summary of Work Package 1\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This work package aims at developing methods for cleaning and preprocessing the data, and select the most relevant features for further model development.\n",
    "\n",
    "## Tasks\n",
    "\n",
    "- Task 1: Read and understand the data\n",
    "  - Understand how to use the provided utility function to read the data\n",
    "  - Understand the data structure and the meaning of each column\n",
    "- Task 2: Clean and preprocess the data\n",
    "  - Explore the quality of the data\n",
    "  - Develop methods to remove the outlier and clean the data\n",
    "- Task 3: Feature engineering\n",
    "  - Visualize the data\n",
    "  - Develop methods to select the most relevant features\n",
    "\n",
    "## Delierables\n",
    "\n",
    "- A Jupyter notebook reporting the process and results of the above tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting, please:\n",
    "- Fetch the most up-to-date version of the github repository.\n",
    "- Create a new branch with your name, based on the \"main\" branch and switch to your own branch.\n",
    "- Copy this notebook to the work space of your group, and rename it to TD_WP_1_Your name.ipynb\n",
    "- After finishing this task, push your changes to the github repository of your group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 Read and understand the data\n",
    "\n",
    "## Sub-task 1: Read the data from the different csv files and store it in a dataframe. \n",
    "\n",
    "The training data is stored in the following path: `projects\\maintenance_industry_4_2024\\dataset\\training_data`. In this folder, there are some subfolders. Each subfolder contains the data of one test. The name of the subfolder tells you the time when the test is performed. Inside each subfolder, there are six csv files, each file corresponds to the data from one motor. The file `Test condition.xlsx` tells you the test condition for each test.\n",
    "\n",
    "In `utility.py`, there are supporting functions `read_all_csvs_one_test` and `read_all_test_data_from_path`. Please have a look at the demo in `demo_read_data_and_preprocess.ipynb`, and use these supporting functions to read the data.\n",
    "\n",
    "\n",
    "**Your task:**\n",
    "\n",
    "- Create a dataframe to store data from all the test.\n",
    "- Write a paragraph to explain the structure of the resulted dataframe (what does each column represent?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain the structure (what does each column represent) of the resulted dataframe here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 2: Data visualization\n",
    "\n",
    "Visualize the data. By examing the data, you can answer the following questions:\n",
    "- Do we have a lot noise in the features? Do we need to smooth the data?\n",
    "- Do we need to scale the features?\n",
    "- Do we have a lot of outliers? Do we need to remove them?\n",
    "- Do we have a lot of missing values? Do we need to fill them?\n",
    "- In general, do you discover some patterns regarding how temperature of the motor changes?\n",
    "- If we compare the normal data (label = 0) with the failures (label=1), could you see some difference in the pattern of the temperature?\n",
    "\n",
    "**Submit your summary below:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 3 Explore the distribution of each feature through histogram and box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sub-task 4 Use PCA to visulize the data in a 2-d plane.\n",
    "\n",
    "Normally, before any further analysis, we try to plot all the features directly in a 2-d plane. This is because the 2-d plane is a very simple representation of the data and allows us to visually inspect the patterns of the data. A normal conclusion we can make is whether the data are linear seperable or not, i.e., if you can simply fit a straight line to seperate postitive and negative classes. However, if the data is high-dimensional, it is difficult to visualize it in a 2-d plane. In this case, we can use PCA to reduce the dimensionality of the data and then plot it in a 2-d plane.\n",
    "\n",
    "Below is a code for performing PCA and plotting the data in a 2-d plane. What can you see from the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X = df.drop(['label', 'sequence_idx', 'time'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Perform PCA to reduce the dimensionality to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], color='red', marker='^', alpha=.5, label='Class 0')\n",
    "plt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], color='blue', marker='o', alpha=.5, label='Class 1')\n",
    "plt.title('2D PCA Result')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above analysis, we did not perform data normalization/standardization. Could you try to do that and see if the results change? You can use the follwing code for standardization:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "```\n",
    "\n",
    "And this for normalization:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Data cleaning and preprocessing\n",
    "\n",
    "## Task 1: Normalize the dataset.\n",
    "\n",
    "Define your strategy to normalize the dataset and implement the data. Please pay attention to the difference between fit_transform(), fit(), and transform()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Define a strategy to remove outliers.\n",
    "\n",
    "Explain how you remove the outliers and implement your approach here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Define a strategy to smooth the data.\n",
    "\n",
    "Explain how do you smooth the data and implement your approach here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Feature engineering\n",
    "\n",
    "## Task 1: Use violin plot to explore the significance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Use correlation coefficient matrix to explore the correlation among the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of feature engineering\n",
    "\n",
    "**Write your conclusion of feature engineering here.** For example, which features are most relevant? Which features are not relevant? Which features are redundant? Which features are irrelevant? Eventually, what are your decisions regarding which features to be used?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
